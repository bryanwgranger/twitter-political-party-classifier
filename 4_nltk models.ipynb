{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "with open('stopwords.txt') as infile:\n",
    "    stop_words = infile.readlines()\n",
    "    stop_words.extend(['rt', '&amp;'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a dictionary sorted  descending by its values\n",
    "def sort_dict(d):\n",
    "    return {k: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "#creates a dictionary of dictionaries that iterates through texts, counting the amount of words in each tweet per 'handle' or user\n",
    "#returns a dictionary of dicitonaries\n",
    "def create_dict(df, handles):\n",
    "    \n",
    "    big_dict = {}\n",
    "    #currently retweets are treated as tweets from the senator\n",
    "    for handle in handles: \n",
    "        handle_dict = defaultdict(int)\n",
    "        for tweet in df[df['handle'] == handle]['tokens']:\n",
    "            for word in tweet:\n",
    "                handle_dict[word] += 1\n",
    "        big_dict[handle] = sort_dict(handle_dict)\n",
    "    return big_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns only the top 'n' amount of entries in a sorted dictionary\n",
    "def dict_slice(d, n):\n",
    "    d_sort = sort_dict(d)\n",
    "    return dict(itertools.islice(d_sort.items(), n))\n",
    "\n",
    "#processes a dictionary of dictionaries into one dictionary with total counts of all words\n",
    "#returns a dictionary\n",
    "def get_all_words(d):\n",
    "    t_dict = defaultdict(int)\n",
    "    for handle in d.keys():\n",
    "        for word in d[handle].keys():\n",
    "            t_dict[word] += d[handle][word]\n",
    "    return sort_dict(t_dict)\n",
    "\n",
    "def bag(series, words):\n",
    "    mat = np.zeros((len(df), len(words)+1))\n",
    "    \n",
    "    wordtoID = {}\n",
    "    for i, word in enumerate(words):\n",
    "        wordtoID[word.strip().lower()] = i\n",
    "    \n",
    "    for i, tweet in enumerate(series):\n",
    "        for word in tweet:\n",
    "            if word.strip().lower() in words:\n",
    "                j = wordtoID[word.strip().lower()]\n",
    "                mat[i][j] += 1\n",
    "        if df['party'].iloc[i] == 'd':\n",
    "            mat[i][-1] = 1\n",
    "            \n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts a string into tokens, removes specific types of words based on part of speech, then stems for each token\n",
    "#returns a list of stems\n",
    "ps = PorterStemmer()\n",
    "def get_tokens(str):\n",
    "    tweet = ''\n",
    "    for word in str.split():\n",
    "        if not word.startswith('http') and word.lower() not in stop_words:\n",
    "            tweet += word + ' '\n",
    "    tokens = word_tokenize(tweet)\n",
    "    pos = pos_tag(tokens)\n",
    "    include_tags = ['VBN', 'VBD', 'JJ', 'JJS', 'JJR', 'CD', 'NN', 'NNS', 'NNP', 'NNPS']\n",
    "    filtered_tokens = [tok for tok, tag in pos if tag in include_tags]\n",
    "    return [ps.stem(tok).lower() for tok in filtered_tokens if len(tok) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('raw_sen.csv', index_col=[0]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['tweet'].apply(get_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_tokens_dict = create_dict(df, df['handle'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_d = get_all_words(sen_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_d_5000 = dict_slice(all_words_d, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = bag(df['tokens'], all_words_d_5000.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135246, 5001)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(bag_of_words[:,:-1], bag_of_words[:,-1], random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing score: 0.7814681178279901\n",
      "Training score: 0.7933730307391998\n"
     ]
    }
   ],
   "source": [
    "print(f'Testing score: {gnb.score(X_test, y_test)}')\n",
    "print(f'Training score: {gnb.score(X_train, y_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing score: 0.8206849639181356\n",
      "Training score: 0.8451209653567837\n"
     ]
    }
   ],
   "source": [
    "print(f'Testing score: {lr.score(X_test, y_test)}')\n",
    "print(f'Training score: {lr.score(X_train, y_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l1',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr1 = LogisticRegression(penalty='l1')\n",
    "lr1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing score: 0.8219862770613984\n",
      "Training score: 0.8448744996746653\n"
     ]
    }
   ],
   "source": [
    "print(f'Testing score: {lr1.score(X_test, y_test)}')\n",
    "print(f'Training score: {lr1.score(X_train, y_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing score: 0.8642789542174376\n",
      "Training score: 0.7908787980361615\n"
     ]
    }
   ],
   "source": [
    "print(f'Testing score: {lda.score(X_test, y_test)}')\n",
    "print(f'Training score: {lda.score(X_train, y_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
